{
  "hash": "35dd42b068fc8460a589d41ff4afccf5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Seminar 2\"\nformat: html\n---\n\n\n\n## Overview\n\nThe goal of seminar 2 is to review the questions in Problem Set 1. Many of these questions do no require R and will be discussed in person during class. For this reason, I have decided to include just some material related to:\n\n**Question 11:** Try repeating exercises with another dataset available here: <https://www.statlearning.com/resources-second-edition>. \n\nI have chosen to use the file Credit.csv, which includes the debt levels of 400 individuals. \n\n## Load packages and data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(glmnet)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'glmnet' was built under R version 4.4.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoaded glmnet 4.1-8\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tree)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'tree' was built under R version 4.4.2\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(randomForest)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nAttaching package: 'randomForest'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n```\n\n\n:::\n\n```{.r .cell-code}\ncredit.base <- read.csv(\"seminar-material/Credit.csv\",header=TRUE, stringsAsFactors=TRUE)\n```\n:::\n\n\n\n## Create training and testing database\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\ntrain <- sample(1:nrow(credit.base), 3*nrow(credit.base)/4)\n\n# Create training data\ncredit.train <- credit.base[train,]\ncredit.trainX <- credit.train[,-ncol(credit.train)]\ncredit.trainY <- credit.train[,ncol(credit.train)]\n\n# Create testing data\ncredit.test <- credit.base[-train,]\ncredit.testX <- credit.test[,-ncol(credit.train)]\ncredit.testY <- credit.test[,ncol(credit.train)]\n```\n:::\n\n\n\n## Linear regression\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm.credit <- lm(Balance ~ ., data = credit.train)\nsummary(lm.credit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Balance ~ ., data = credit.train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-142.12  -72.93  -15.53   49.04  328.78 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -484.8236    39.9566 -12.134  < 2e-16 ***\nIncome        -7.7615     0.2725 -28.484  < 2e-16 ***\nLimit          0.2426     0.0364   6.664 1.36e-10 ***\nRating         0.4109     0.5474   0.751   0.4534    \nCards         22.4444     4.8474   4.630 5.53e-06 ***\nAge           -0.7274     0.3317  -2.193   0.0291 *  \nEducation      0.0567     1.7870   0.032   0.9747    \nOwnYes       -19.3752    11.0876  -1.747   0.0816 .  \nStudentYes   417.2180    17.9747  23.211  < 2e-16 ***\nMarriedYes    -4.2692    11.7505  -0.363   0.7166    \nRegionSouth   -0.8237    13.7553  -0.060   0.9523    \nRegionWest    15.5498    16.2218   0.959   0.3386    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 95.65 on 288 degrees of freedom\nMultiple R-squared:  0.9589,\tAdjusted R-squared:  0.9574 \nF-statistic: 611.2 on 11 and 288 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nNow we can compute the MSE\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm.pred <- predict(lm.credit, newdata = credit.testX)\nMSE.lm <- mean((lm.pred - credit.testY)^2)\n```\n:::\n\n\n\n## Ridge regression\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nridge.credit <- glmnet(credit.trainX, credit.trainY, alpha=0, lamnda=3, thresh = 1e-12)\n#coef(ridge.credit)\n```\n:::\n\n\n\nAdding cross-validation\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncv.out <- cv.glmnet(as.matrix(credit.trainX),as.matrix(credit.trainY), alpha=0, nfold=3)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in storage.mode(xd) <- \"double\": NAs introduced by coercion\nWarning in storage.mode(xd) <- \"double\": NAs introduced by coercion\nWarning in storage.mode(xd) <- \"double\": NAs introduced by coercion\nWarning in storage.mode(xd) <- \"double\": NAs introduced by coercion\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in cbind2(1, newx) %*% nbeta: NAs introduced by coercion\nWarning in cbind2(1, newx) %*% nbeta: NAs introduced by coercion\nWarning in cbind2(1, newx) %*% nbeta: NAs introduced by coercion\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(cv.out)\n```\n\n::: {.cell-output-display}\n![](seminar-2_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\nlambda.ridge.cv <- cv.out$lambda.min\n```\n:::\n\n\n\nRe-estimate using cross-validated lambda\n\n\n::: {.cell}\n\n```{.r .cell-code}\nridge.credit <- glmnet(credit.trainX, credit.trainY, alpha=0, lamnda=lambda.ridge.cv, thresh = 1e-12)\n```\n:::\n\n\n\nFit the model in the test data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nridge.pred <- predict(ridge.credit, s = lambda.ridge.cv, newx = as.matrix(credit.testX))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in cbind2(1, newx) %*% nbeta: NAs introduced by coercion\n```\n\n\n:::\n\n```{.r .cell-code}\nMSE.ridge <- mean((ridge.pred - credit.testY)^2)\n```\n:::\n\n\n\n## LASSO\nRepeat the above steps with cross-validation, but setting `alpha=1`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncv.out <- cv.glmnet(as.matrix(credit.trainX),as.matrix(credit.trainY), alpha=1, nfold=3)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in storage.mode(xd) <- \"double\": NAs introduced by coercion\nWarning in storage.mode(xd) <- \"double\": NAs introduced by coercion\nWarning in storage.mode(xd) <- \"double\": NAs introduced by coercion\nWarning in storage.mode(xd) <- \"double\": NAs introduced by coercion\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in cbind2(1, newx) %*% nbeta: NAs introduced by coercion\nWarning in cbind2(1, newx) %*% nbeta: NAs introduced by coercion\nWarning in cbind2(1, newx) %*% nbeta: NAs introduced by coercion\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(cv.out)\n```\n\n::: {.cell-output-display}\n![](seminar-2_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\nlambda.LASSO.cv <- cv.out$lambda.min\n```\n:::\n\n\n\nRe-estimate using cross-validated lambda\n\n\n::: {.cell}\n\n```{.r .cell-code}\nLASSO.credit <- glmnet(credit.trainX, credit.trainY, alpha=0, lamnda=lambda.LASSO.cv, thresh = 1e-12)\n```\n:::\n\n\n\nFit the model in the test data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nLASSO.pred <- predict(LASSO.credit, s = lambda.LASSO.cv, newx = as.matrix(credit.testX))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in cbind2(1, newx) %*% nbeta: NAs introduced by coercion\n```\n\n\n:::\n\n```{.r .cell-code}\nMSE.LASSO <- mean((LASSO.pred - credit.testY)^2)\n```\n:::\n\n\n\n## Regression Trees\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree.credit <- tree(Balance ~ ., data = credit.train)\nsummary(tree.credit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRegression tree:\ntree(formula = Balance ~ ., data = credit.train)\nVariables actually used in tree construction:\n[1] \"Rating\"  \"Income\"  \"Student\" \"Limit\"  \nNumber of terminal nodes:  9 \nResidual mean deviance:  29060 = 8457000 / 291 \nDistribution of residuals:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-672.40  -70.32  -18.64    0.00  107.60  484.60 \n```\n\n\n:::\n\n```{.r .cell-code}\ntree.credit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 300 64150000  528.70  \n   2) Rating < 353.5 163  8879000  199.10  \n     4) Rating < 278.5 99  1377000   70.32 *\n     5) Rating > 278.5 64  3320000  398.30  \n      10) Income < 45.049 51  1828000  473.70  \n        20) Student: No 45   902100  427.10 *\n        21) Student: Yes 6    93940  823.50 *\n      11) Income > 45.049 13    63310  102.30 *\n   3) Rating > 353.5 137 16500000  920.80  \n     6) Rating < 717.5 126 10880000  863.90  \n      12) Limit < 5353 39  1253000  618.90  \n        24) Income < 48.3975 27   424300  708.00 *\n        25) Income > 48.3975 12   132600  418.50 *\n      13) Limit > 5353 87  6239000  973.80  \n        26) Student: No 74  4220000  922.40 *\n        27) Student: Yes 13   709200 1266.00 *\n     7) Rating > 717.5 11   534000 1573.00 *\n```\n\n\n:::\n:::\n\n\nWe can plot the tree as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree.pred <- predict(tree.credit, newdata = credit.test)\nMSE.tree <- mean((tree.pred - credit.testY)^2)\nplot(tree.credit)\n  text(tree.credit , pretty = 1)\n```\n\n::: {.cell-output-display}\n![](seminar-2_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\nA simpler tree with only two variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree.credit <- tree(Balance ~ Age + Limit, data = credit.train)\npartition.tree(tree.credit)\npoints(credit.train[, c(\"Age\", \"Limit\")], cex = .4)\n```\n\n::: {.cell-output-display}\n![](seminar-2_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(tree.credit)\n text(tree.credit, pretty = 1)\n title(main = \"Unpruned Regression Tree\")\n```\n\n::: {.cell-output-display}\n![](seminar-2_files/figure-html/unnamed-chunk-14-2.png){width=672}\n:::\n:::\n\n\n\nPruned tree (following example on P.355):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(789)\ncvtree.credit <- cv.tree(tree.credit, FUN = prune.tree)\nnames(cvtree.credit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"size\"   \"dev\"    \"k\"      \"method\"\n```\n\n\n:::\n\n```{.r .cell-code}\ncvtree.credit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$size\n[1] 5 4 3 2 1\n\n$dev\n[1] 15926768 18008409 20028897 25157250 64799501\n\n$k\n[1]     -Inf  2564445  3322485  7162302 38604569\n\n$method\n[1] \"deviance\"\n\nattr(,\"class\")\n[1] \"prune\"         \"tree.sequence\"\n```\n\n\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 2))\nplot(cvtree.credit$size , cvtree.credit$dev, type = \"b\")\nplot(cvtree.credit$k, cvtree.credit$dev, type = \"b\")\n```\n\n::: {.cell-output-display}\n![](seminar-2_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\nPrune the tree, predict and compute MSE, and plot new tree\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprune.credit <- prune.tree(tree.credit , best = 9)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in prune.tree(tree.credit, best = 9): best is bigger than tree size\n```\n\n\n:::\n\n```{.r .cell-code}\nprune.pred <- predict(prune.credit, newdata = credit.test)\nMSE.prune <- mean((prune.pred - credit.testY)^2)\nplot(prune.credit)\n text(prune.credit , pretty = 1)\n```\n\n::: {.cell-output-display}\n![](seminar-2_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n## Bagging\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(8)\nbag.credit <- randomForest(Balance ~ . , data= credit.train,mtry = 10, importance = TRUE)\nbag.credit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\n randomForest(formula = Balance ~ ., data = credit.train, mtry = 10,      importance = TRUE) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 10\n\n          Mean of squared residuals: 14245.24\n                    % Var explained: 93.34\n```\n\n\n:::\n\n```{.r .cell-code}\nbag.pred <- predict(bag.credit , newdata = credit.test)\nplot(bag.pred , credit.testY)\n abline(0, 1)\n```\n\n::: {.cell-output-display}\n![](seminar-2_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n\n```{.r .cell-code}\nMSE.bag <- mean((bag.pred - credit.testY)^2)\n```\n:::\n\n\n\n## Random Forest\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(9)\nforest.credit <- randomForest(Balance ~ . , data= credit.train,mtry = 10, importance = TRUE)\nforest.pred <- predict(forest.credit, newdata = credit.test)\nMSE.forest <- mean((forest.pred - credit.testY)^2)\n```\n:::\n\n\n\nWe can view the importance of each variable:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimportance(forest.credit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             %IncMSE IncNodePurity\nIncome    65.6344496    4435353.87\nLimit     44.5100467   34575258.49\nRating    26.2543973   20230908.15\nCards      1.7636025     178265.35\nAge        6.1622703     429923.12\nEducation  4.1044398     378287.40\nOwn       -1.8976022      57244.49\nStudent   55.5654832    2778949.90\nMarried   -0.2942654      99967.08\nRegion    -2.1374350     134736.65\n```\n\n\n:::\n\n```{.r .cell-code}\nvarImpPlot(forest.credit)\n```\n\n::: {.cell-output-display}\n![](seminar-2_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\n## Comparison\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nMSE <- c(LM = MSE.lm, Ridge = MSE.ridge, LASSO = MSE.LASSO, PrunedTree = MSE.prune, Bag = MSE.bag, Forest = MSE.forest)\nt(t(MSE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               [,1]\nLM         12361.96\nRidge            NA\nLASSO            NA\nPrunedTree 69499.81\nBag        12386.55\nForest     12738.86\n```\n\n\n:::\n:::",
    "supporting": [
      "seminar-2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}