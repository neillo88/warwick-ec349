{
  "hash": "875bdeeec7424b121f11e0c8353a41cc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Seminar 2\"\nformat: html\n---\n\n\n\n## Overview\n\nThe goal of seminar 2 is to review the questions in Problem Set 1. Many of these questions do no require R and a selection will be discussed in person during class. Here, you will find an initial attempt at Q11.\n\n**Question 11:** Try repeating exercises with another dataset available here: <https://www.statlearning.com/resources-second-edition>.\n\nFor this exercise, I have chosen to use the file Credit.csv, which includes the debt levels of 400 individuals. The exercise will be predict the credit-balance of card holders using the other information in the file.\n\n::: {#exr-q11}\nDownload one of the datasets and apply each of the models below. In addition, try to improve on my code by using functions in `tidyverse` package. For example, look at this [example](https://www.geeksforgeeks.org/logistic-regression-in-r-programming/?ref=header_outind) that uses `dplyr` package to create the training and testing data. \n:::\n\n## Load packages and data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(glmnet)\nlibrary(tree)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(randomForest)\n\n# read in csv\ncredit.base <- read.csv(\"seminar-material/Credit.csv\",header=TRUE, stringsAsFactors=TRUE)\n```\n:::\n\n\n\n## Create training and testing database\n\nThe outcome of interest is \"Balance\", which appears as the last variable in the data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\ntrain <- sample(1:nrow(credit.base), 3*nrow(credit.base)/4)\n\n# Create training data\ncredit.train <- credit.base[train,]\ncredit.trainX <- credit.train[,-ncol(credit.train)]\ncredit.trainY <- credit.train[,ncol(credit.train)]\n\n# Create testing data\ncredit.test <- credit.base[-train,]\ncredit.testX <- credit.test[,-ncol(credit.train)]\ncredit.testY <- credit.test[,ncol(credit.train)]\n```\n:::\n\n\n\n## Linear regression\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm.credit <- lm(Balance ~ ., data = credit.train)\nsummary(lm.credit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Balance ~ ., data = credit.train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-142.12  -72.93  -15.53   49.04  328.78 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -484.8236    39.9566 -12.134  < 2e-16 ***\nIncome        -7.7615     0.2725 -28.484  < 2e-16 ***\nLimit          0.2426     0.0364   6.664 1.36e-10 ***\nRating         0.4109     0.5474   0.751   0.4534    \nCards         22.4444     4.8474   4.630 5.53e-06 ***\nAge           -0.7274     0.3317  -2.193   0.0291 *  \nEducation      0.0567     1.7870   0.032   0.9747    \nOwnYes       -19.3752    11.0876  -1.747   0.0816 .  \nStudentYes   417.2180    17.9747  23.211  < 2e-16 ***\nMarriedYes    -4.2692    11.7505  -0.363   0.7166    \nRegionSouth   -0.8237    13.7553  -0.060   0.9523    \nRegionWest    15.5498    16.2218   0.959   0.3386    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 95.65 on 288 degrees of freedom\nMultiple R-squared:  0.9589,\tAdjusted R-squared:  0.9574 \nF-statistic: 611.2 on 11 and 288 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\nCompute the predicted values and MSE\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm.pred <- predict(lm.credit, newdata = credit.testX)\nplot(lm.pred , credit.testY)\n abline(0, 1)\n```\n\n::: {.cell-output-display}\n![](seminar-2_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\nMSE.lm <- mean((lm.pred - credit.testY)^2)\n```\n:::\n\n\n::: {.callout-note title=\"Non-linear models\"}\nFor discrete outcomes, see probit/logit models: <https://www.geeksforgeeks.org/logistic-regression-in-r-programming/?ref=header_outind>. And for categorical variables, see multinomial logit models: <https://www.geeksforgeeks.org/multinomial-logistic-regression-in-r/>. This resource uses the `vglm` function.\n:::\n## Ridge regression\n\nThe dataset contains factor variables: these have numerical values with labels attached (e.g. \"Yes\",\"No\"). When using a function like `lm()` it will convert this two a set of dummy variables.\n\n::: {.callout-warning title=\"Matrices with factor variables\"}\nThe `glmnet` function wants you to input a Y and X matrix. I had trouble using the `as.matrix()` function with the factor variables. As a solution (courtesy of ChatGPT), I first convert the X's into a matrix where the factor variable appear as dummy variables.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncredit.trainX.mat <- model.matrix(~ ., data = credit.trainX)[, -1]\ncredit.testX.mat <- model.matrix(~ ., data = credit.testX)[, -1]\n```\n:::\n\n\n\nThe `as.matrix` function works fine for the outcome variable. We can now estimate the model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nridge.credit <- glmnet(credit.trainX.mat, as.matrix(credit.trainY), alpha=0, lamnda=3, thresh = 1e-12)\n#coef(ridge.credit)\n```\n:::\n\n\n\nAdding cross-validation\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncv.out <- cv.glmnet(credit.trainX.mat,as.matrix(credit.trainY), alpha=0, nfold=3)\nplot(cv.out)\n```\n\n::: {.cell-output-display}\n![](seminar-2_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\nlambda.ridge.cv <- cv.out$lambda.min\n```\n:::\n\n\n\nRe-estimate using cross-validated lambda\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nridge.credit <- glmnet(credit.trainX.mat, as.matrix(credit.trainY), alpha=0, lamnda=lambda.ridge.cv, thresh = 1e-12)\n```\n:::\n\n\n\nFit the model in the test data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nridge.pred <- predict(ridge.credit, s = lambda.ridge.cv, newx = credit.testX.mat)\nplot(ridge.pred , credit.testY)\n abline(0, 1)\n```\n\n::: {.cell-output-display}\n![](seminar-2_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\nMSE.ridge <- mean((ridge.pred - credit.testY)^2)\n```\n:::\n\n\n\n## LASSO\n\nRepeat the above steps with cross-validation, but setting `alpha=1`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncv.out <- cv.glmnet(credit.trainX.mat,as.matrix(credit.trainY), alpha=1, nfold=3)\nplot(cv.out)\n```\n\n::: {.cell-output-display}\n![](seminar-2_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\nlambda.LASSO.cv <- cv.out$lambda.min\n```\n:::\n\n\n\nRe-estimate using cross-validated lambda\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nLASSO.credit <- glmnet(credit.trainX.mat, as.matrix(credit.trainY), alpha=0, lamnda=lambda.LASSO.cv, thresh = 1e-12)\n```\n:::\n\n\n\nFit the model in the test data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nLASSO.pred <- predict(LASSO.credit, s = lambda.LASSO.cv, newx = credit.testX.mat)\nplot(LASSO.pred , credit.testY)\n abline(0, 1)\n```\n\n::: {.cell-output-display}\n![](seminar-2_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code}\nMSE.LASSO <- mean((LASSO.pred - credit.testY)^2)\n```\n:::\n\n\n\n## Regression Trees\n\nI first tried following the coded examples in James *et al.* (2023) Chapter 8. However, the pruning process was not clear. Next, I followed the advice of <https://www.geeksforgeeks.org/how-to-prune-a-tree-in-r/> using the `rpart` package.\n\n### Version 1\n\nHere is the first version using the `tree` package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree.credit <- tree(Balance ~ ., data = credit.train)\nsummary(tree.credit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRegression tree:\ntree(formula = Balance ~ ., data = credit.train)\nVariables actually used in tree construction:\n[1] \"Rating\"  \"Income\"  \"Student\" \"Limit\"  \nNumber of terminal nodes:  9 \nResidual mean deviance:  29060 = 8457000 / 291 \nDistribution of residuals:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-672.40  -70.32  -18.64    0.00  107.60  484.60 \n```\n\n\n:::\n\n```{.r .cell-code}\ntree.credit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 300 64150000  528.70  \n   2) Rating < 353.5 163  8879000  199.10  \n     4) Rating < 278.5 99  1377000   70.32 *\n     5) Rating > 278.5 64  3320000  398.30  \n      10) Income < 45.049 51  1828000  473.70  \n        20) Student: No 45   902100  427.10 *\n        21) Student: Yes 6    93940  823.50 *\n      11) Income > 45.049 13    63310  102.30 *\n   3) Rating > 353.5 137 16500000  920.80  \n     6) Rating < 717.5 126 10880000  863.90  \n      12) Limit < 5353 39  1253000  618.90  \n        24) Income < 48.3975 27   424300  708.00 *\n        25) Income > 48.3975 12   132600  418.50 *\n      13) Limit > 5353 87  6239000  973.80  \n        26) Student: No 74  4220000  922.40 *\n        27) Student: Yes 13   709200 1266.00 *\n     7) Rating > 717.5 11   534000 1573.00 *\n```\n\n\n:::\n:::\n\n\n\nPlot the tree\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(tree.credit)\n  text(tree.credit , pretty = 1)\n```\n\n::: {.cell-output-display}\n![](seminar-2_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\nCompute the predicted values and MSE:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree.pred <- predict(tree.credit, newdata = credit.test)\nplot(tree.pred , credit.testY)\n abline(0, 1)\n```\n\n::: {.cell-output-display}\n![](seminar-2_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n\n```{.r .cell-code}\nMSE.tree <- mean((tree.pred - credit.testY)^2)\n```\n:::\n\n\n\nPruned tree (following example on P.355):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(789)\ncvtree.credit <- cv.tree(tree.credit, FUN = prune.tree)\nnames(cvtree.credit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"size\"   \"dev\"    \"k\"      \"method\"\n```\n\n\n:::\n\n```{.r .cell-code}\ncvtree.credit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$size\n[1] 9 8 7 6 5 4 3 2 1\n\n$dev\n[1] 12853234 13736173 13923420 15697812 16415511 22200274 24575504 25292336\n[9] 64799501\n\n$k\n[1]       -Inf   696091.6   831927.0  1309232.5  1429088.0  3391459.1  4180845.0\n[8]  5079908.9 38774711.8\n\n$method\n[1] \"deviance\"\n\nattr(,\"class\")\n[1] \"prune\"         \"tree.sequence\"\n```\n\n\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 2))\nplot(cvtree.credit$size , cvtree.credit$dev, type = \"b\")\nplot(cvtree.credit$k, cvtree.credit$dev, type = \"b\")\n```\n\n::: {.cell-output-display}\n![](seminar-2_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\nPrune the tree, predict and compute MSE, and plot new tree\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprune.credit <- prune.tree(tree.credit , best = 8)\nplot(prune.credit)\n text(prune.credit , pretty = 1)\n```\n\n::: {.cell-output-display}\n![](seminar-2_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\nCompute predicted values\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprune.pred <- predict(prune.credit, newdata = credit.test)\nplot(prune.pred , credit.testY)\n abline(0, 1)\n```\n\n::: {.cell-output-display}\n![](seminar-2_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n\n```{.r .cell-code}\nMSE.prune <- mean((prune.pred - credit.testY)^2)\n```\n:::\n\n\n\n### Version 2\nNext, using the `rpart` package. I get the following:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree.credit2 <- rpart(Balance ~ ., data = credit.train, method = \"anova\")\nsummary(tree.credit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nrpart(formula = Balance ~ ., data = credit.train, method = \"anova\")\n  n= 300 \n\n          CP nsplit rel error    xerror       xstd\n1 0.60443234      0 1.0000000 1.0073204 0.07254634\n2 0.07918721      1 0.3955677 0.4664061 0.04213623\n3 0.06517232      2 0.3163805 0.3434325 0.03249794\n4 0.05286712      3 0.2512081 0.3167628 0.03079975\n5 0.02227707      4 0.1983410 0.2465855 0.02432872\n6 0.02040873      5 0.1760639 0.2455674 0.02409388\n7 0.01200212      6 0.1556552 0.2378676 0.02362119\n8 0.01085089      8 0.1316510 0.2156099 0.02410439\n9 0.01000000      9 0.1208001 0.2123373 0.02369780\n\nVariable importance\n   Rating     Limit    Income       Age Education     Cards   Student \n       39        37        18         3         1         1         1 \n\nNode number 1: 300 observations,    complexity param=0.6044323\n  mean=528.6867, MSE=213835.4 \n  left son=2 (163 obs) right son=3 (137 obs)\n  Primary splits:\n      Rating  < 353.5    to the left,  improve=0.60443230, (0 missing)\n      Limit   < 4421     to the left,  improve=0.60178010, (0 missing)\n      Income  < 58.566   to the left,  improve=0.20501110, (0 missing)\n      Student splits as  LR,           improve=0.04117036, (0 missing)\n      Age     < 58.5     to the right, improve=0.01333096, (0 missing)\n  Surrogate splits:\n      Limit     < 4765.5   to the left,  agree=0.980, adj=0.956, (0 split)\n      Income    < 45.5635  to the left,  agree=0.747, adj=0.445, (0 split)\n      Age       < 79.5     to the left,  agree=0.583, adj=0.088, (0 split)\n      Cards     < 1.5      to the right, agree=0.553, adj=0.022, (0 split)\n      Education < 7.5      to the right, agree=0.553, adj=0.022, (0 split)\n\nNode number 2: 163 observations,    complexity param=0.06517232\n  mean=199.092, MSE=54470.16 \n  left son=4 (99 obs) right son=5 (64 obs)\n  Primary splits:\n      Rating  < 278.5    to the left,  improve=0.47088820, (0 missing)\n      Limit   < 3570.5   to the left,  improve=0.46202030, (0 missing)\n      Student splits as  LR,           improve=0.12815030, (0 missing)\n      Income  < 15.0005  to the right, improve=0.05136852, (0 missing)\n      Age     < 31.5     to the right, improve=0.03893959, (0 missing)\n  Surrogate splits:\n      Limit     < 3429     to the left,  agree=0.945, adj=0.859, (0 split)\n      Income    < 39.1305  to the left,  agree=0.687, adj=0.203, (0 split)\n      Age       < 31.5     to the right, agree=0.626, adj=0.047, (0 split)\n      Cards     < 5.5      to the left,  agree=0.620, adj=0.031, (0 split)\n      Education < 9.5      to the right, agree=0.620, adj=0.031, (0 split)\n\nNode number 3: 137 observations,    complexity param=0.07918721\n  mean=920.8321, MSE=120418.1 \n  left son=6 (126 obs) right son=7 (11 obs)\n  Primary splits:\n      Rating    < 717.5    to the left,  improve=0.30792410, (0 missing)\n      Limit     < 8922.5   to the left,  improve=0.30596580, (0 missing)\n      Income    < 134.6495 to the left,  improve=0.12713150, (0 missing)\n      Student   splits as  LR,           improve=0.09558537, (0 missing)\n      Education < 13.5     to the left,  improve=0.03174229, (0 missing)\n  Surrogate splits:\n      Limit  < 9956     to the left,  agree=0.993, adj=0.909, (0 split)\n      Income < 150.807  to the left,  agree=0.956, adj=0.455, (0 split)\n      Cards  < 6.5      to the left,  agree=0.927, adj=0.091, (0 split)\n      Age    < 85.5     to the left,  agree=0.927, adj=0.091, (0 split)\n\nNode number 4: 99 observations\n  mean=70.32323, MSE=13913.81 \n\nNode number 5: 64 observations,    complexity param=0.02227707\n  mean=398.2812, MSE=51880.05 \n  left son=10 (13 obs) right son=11 (51 obs)\n  Primary splits:\n      Income < 45.049   to the right, improve=0.43040630, (0 missing)\n      Limit  < 4421     to the left,  improve=0.12565140, (0 missing)\n      Age    < 72.5     to the right, improve=0.09399284, (0 missing)\n      Rating < 327.5    to the left,  improve=0.05022884, (0 missing)\n      Cards  < 3.5      to the left,  improve=0.04388043, (0 missing)\n  Surrogate splits:\n      Age < 79.5     to the right, agree=0.859, adj=0.308, (0 split)\n\nNode number 6: 126 observations,    complexity param=0.05286712\n  mean=863.9365, MSE=86375.81 \n  left son=12 (39 obs) right son=13 (87 obs)\n  Primary splits:\n      Limit     < 5353     to the left,  improve=0.31161900, (0 missing)\n      Rating    < 406.5    to the left,  improve=0.25619670, (0 missing)\n      Student   splits as  LR,           improve=0.20599790, (0 missing)\n      Education < 8.5      to the left,  improve=0.06908941, (0 missing)\n      Age       < 70       to the right, improve=0.02733715, (0 missing)\n  Surrogate splits:\n      Rating    < 403.5    to the left,  agree=0.944, adj=0.821, (0 split)\n      Education < 18.5     to the right, agree=0.722, adj=0.103, (0 split)\n      Income    < 20.1615  to the left,  agree=0.706, adj=0.051, (0 split)\n\nNode number 7: 11 observations\n  mean=1572.545, MSE=48546.98 \n\nNode number 10: 13 observations\n  mean=102.3077, MSE=4870.213 \n\nNode number 11: 51 observations\n  mean=473.7255, MSE=35841.61 \n\nNode number 12: 39 observations,    complexity param=0.01085089\n  mean=618.8974, MSE=32128.55 \n  left son=24 (12 obs) right son=25 (27 obs)\n  Primary splits:\n      Income    < 48.3975  to the right, improve=0.55553400, (0 missing)\n      Education < 10.5     to the left,  improve=0.18867870, (0 missing)\n      Age       < 70       to the right, improve=0.10403420, (0 missing)\n      Limit     < 4942     to the right, improve=0.08655423, (0 missing)\n      Region    splits as  RLL,          improve=0.07476262, (0 missing)\n  Surrogate splits:\n      Limit     < 5291.5   to the right, agree=0.769, adj=0.250, (0 split)\n      Education < 9.5      to the left,  agree=0.769, adj=0.250, (0 split)\n      Rating    < 388      to the right, agree=0.744, adj=0.167, (0 split)\n      Age       < 70       to the right, agree=0.718, adj=0.083, (0 split)\n\nNode number 13: 87 observations,    complexity param=0.02040873\n  mean=973.7816, MSE=71711.25 \n  left son=26 (74 obs) right son=27 (13 obs)\n  Primary splits:\n      Student   splits as  LR,           improve=0.20985060, (0 missing)\n      Income    < 72.5045  to the right, improve=0.13136250, (0 missing)\n      Education < 13.5     to the left,  improve=0.07140088, (0 missing)\n      Age       < 70       to the right, improve=0.06687376, (0 missing)\n      Limit     < 7854.5   to the left,  improve=0.06149053, (0 missing)\n  Surrogate splits:\n      Rating < 385.5    to the right, agree=0.874, adj=0.154, (0 split)\n      Limit  < 5386     to the right, agree=0.862, adj=0.077, (0 split)\n\nNode number 24: 12 observations\n  mean=418.5, MSE=11049.92 \n\nNode number 25: 27 observations\n  mean=707.963, MSE=15715.67 \n\nNode number 26: 74 observations,    complexity param=0.01200212\n  mean=922.3649, MSE=57033.69 \n  left son=52 (60 obs) right son=53 (14 obs)\n  Primary splits:\n      Limit     < 7854.5   to the left,  improve=0.14294230, (0 missing)\n      Income    < 83.9085  to the right, improve=0.14144100, (0 missing)\n      Education < 13.5     to the left,  improve=0.11730300, (0 missing)\n      Rating    < 563.5    to the left,  improve=0.11369130, (0 missing)\n      Age       < 81.5     to the right, improve=0.08734887, (0 missing)\n  Surrogate splits:\n      Rating < 563.5    to the left,  agree=0.986, adj=0.929, (0 split)\n      Income < 128.3545 to the left,  agree=0.865, adj=0.286, (0 split)\n\nNode number 27: 13 observations\n  mean=1266.462, MSE=54550.25 \n\nNode number 52: 60 observations,    complexity param=0.01200212\n  mean=878.75, MSE=41006.69 \n  left son=104 (9 obs) right son=105 (51 obs)\n  Primary splits:\n      Income    < 86.474   to the right, improve=0.38066960, (0 missing)\n      Rating    < 457      to the left,  improve=0.03416397, (0 missing)\n      Education < 13.5     to the left,  improve=0.03168693, (0 missing)\n      Limit     < 6131     to the left,  improve=0.03136799, (0 missing)\n      Cards     < 3.5      to the left,  improve=0.02967154, (0 missing)\n  Surrogate splits:\n      Limit  < 7565.5   to the right, agree=0.917, adj=0.444, (0 split)\n      Rating < 548      to the right, agree=0.883, adj=0.222, (0 split)\n\nNode number 53: 14 observations\n  mean=1109.286, MSE=82628.92 \n\nNode number 104: 9 observations\n  mean=581.3333, MSE=41310.44 \n\nNode number 105: 51 observations\n  mean=931.2353, MSE=22588.38 \n```\n\n\n:::\n\n```{.r .cell-code}\ntree.credit2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nn= 300 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n  1) root 300 64150620.00  528.68670  \n    2) Rating< 353.5 163  8878636.00  199.09200  \n      4) Rating< 278.5 99  1377468.00   70.32323 *\n      5) Rating>=278.5 64  3320323.00  398.28120  \n       10) Income>=45.049 13    63312.77  102.30770 *\n       11) Income< 45.049 51  1827922.00  473.72550 *\n    3) Rating>=353.5 137 16497280.00  920.83210  \n      6) Rating< 717.5 126 10883350.00  863.93650  \n       12) Limit< 5353 39  1253014.00  618.89740  \n         24) Income>=48.3975 12   132599.00  418.50000 *\n         25) Income< 48.3975 27   424323.00  707.96300 *\n       13) Limit>=5353 87  6238879.00  973.78160  \n         26) Student=No 74  4220493.00  922.36490  \n           52) Limit< 7854.5 60  2460401.00  878.75000  \n            104) Income>=86.474 9   371794.00  581.33330 *\n            105) Income< 86.474 51  1152007.00  931.23530 *\n           53) Limit>=7854.5 14  1156805.00 1109.28600 *\n         27) Student=Yes 13   709153.20 1266.46200 *\n      7) Rating>=717.5 11   534016.70 1572.54500 *\n```\n\n\n:::\n:::\n\n\n\nThis package makes nicer plots:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrpart.plot(tree.credit2)\n```\n\n::: {.cell-output-display}\n![](seminar-2_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n\nCompute predicted values and MSE:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree.pred2 <- predict(tree.credit2, newdata = credit.test)\nplot(tree.pred2 , credit.testY)\n abline(0, 1)\n```\n\n::: {.cell-output-display}\n![](seminar-2_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n\n```{.r .cell-code}\nMSE.tree2 <- mean((tree.pred2 - credit.testY)^2)\n```\n:::\n\n\n\nPlot the cost-complexity parameter of the tree\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplotcp(tree.credit2)\n```\n\n::: {.cell-output-display}\n![](seminar-2_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n\nUse cross-validation to pick the optimal cp parameter:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get the optimal cp value\noptimal.cp <- tree.credit2$cptable[which.min(tree.credit2$cptable[,\"xerror\"]), \"CP\"]\n\n# Prune the tree\nprune.credit2 <- prune(tree.credit2, cp = optimal.cp)\n\n# Plot the pruned tree\nrpart.plot(prune.credit2)\n```\n\n::: {.cell-output-display}\n![](seminar-2_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n\nCompute predicted values and MSE for pruned tree:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprune.pred2 <- predict(prune.credit2, newdata = credit.test)\nplot(prune.pred2 , credit.testY)\n abline(0, 1)\n```\n\n::: {.cell-output-display}\n![](seminar-2_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n\n```{.r .cell-code}\nMSE.prune2 <- mean((prune.pred2 - credit.testY)^2)\n```\n:::\n\n\n\n## Bagging\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(8)\nbag.credit <- randomForest(Balance ~ . , data= credit.train,mtry = 10, importance = TRUE)\nbag.credit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\n randomForest(formula = Balance ~ ., data = credit.train, mtry = 10,      importance = TRUE) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 10\n\n          Mean of squared residuals: 14245.24\n                    % Var explained: 93.34\n```\n\n\n:::\n\n```{.r .cell-code}\nbag.pred <- predict(bag.credit , newdata = credit.test)\nplot(bag.pred , credit.testY)\n abline(0, 1)\n```\n\n::: {.cell-output-display}\n![](seminar-2_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n\n```{.r .cell-code}\nMSE.bag <- mean((bag.pred - credit.testY)^2)\n```\n:::\n\n\n\n## Random Forest\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(9)\nforest.credit <- randomForest(Balance ~ . , data= credit.train,mtry = 10, importance = TRUE)\nforest.pred <- predict(forest.credit, newdata = credit.test)\nplot(forest.pred , credit.testY)\n abline(0, 1)\n```\n\n::: {.cell-output-display}\n![](seminar-2_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n\n```{.r .cell-code}\nMSE.forest <- mean((forest.pred - credit.testY)^2)\n```\n:::\n\n\n\nWe can view the importance of each variable:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimportance(forest.credit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             %IncMSE IncNodePurity\nIncome    65.6344496    4435353.87\nLimit     44.5100467   34575258.49\nRating    26.2543973   20230908.15\nCards      1.7636025     178265.35\nAge        6.1622703     429923.12\nEducation  4.1044398     378287.40\nOwn       -1.8976022      57244.49\nStudent   55.5654832    2778949.90\nMarried   -0.2942654      99967.08\nRegion    -2.1374350     134736.65\n```\n\n\n:::\n\n```{.r .cell-code}\nvarImpPlot(forest.credit)\n```\n\n::: {.cell-output-display}\n![](seminar-2_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\n\n## Comparison\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nMSE <- c(LM = MSE.lm, Ridge = MSE.ridge, LASSO = MSE.LASSO, Tree = MSE.tree, PrunedTree = MSE.prune, Tree2 = MSE.tree2, PrunedTree2 = MSE.prune2, Bag = MSE.bag, Forest = MSE.forest)\nt(t(MSE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                [,1]\nLM          12361.96\nRidge       15751.42\nLASSO       15751.42\nTree        39324.74\nPrunedTree  49255.46\nTree2       34343.51\nPrunedTree2 34343.51\nBag         12386.55\nForest      12738.86\n```\n\n\n:::\n:::\n",
    "supporting": [
      "seminar-2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}