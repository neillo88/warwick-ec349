{
  "hash": "74cf4994f6fb253df1b70c59d699477e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Seminar 3\"\nformat: html\n---\n\n\n\n\n## Overview\n\nThe goal of seminar 3 is to review the questions in Problem Set 2. Many of these questions do no require R and a selection will be discussed in person during class. Here, you will find an initial attempt at Q4.\n\n**Question 4:** Please try working through the coding example in the week 6's lecture note on the feedforward neural network.\n\n::: {#exr-q4}\nTry to run the following code on your own computer. You should be able to replicate the results in the slides for a 128-128-128 architecture. Next, modify the code to replicate other architectures.  \n:::\n\n## Load packages and data\n\n::: {.callout-warning title=\"Updating R and installing `tensorflow`\"}\nI had issues with some of the packages and needed to update to the latest version of R (4.2.2.).   \n:::\n\nYou will need to install a python package called `tensorflow`. There are two ways to do this that may need some troubleshooting depending on your set-up. \n\n**Option 1.** Install `tensorflow` in python separately. You can do this easily, using `pip install tensorflow` in the Anaconda Prompt terminal. You will likely be encouraged to install `tensorflow` in a virtual environment. If you do this, you will then need to change the Python environment in R. \n\nYou can do this in two ways:\n\n1. In RStudio, go to Tools>Global Options...>Python. Use the \"Select\" button to change the environment for the current session. \n2. Use the `reticulate` package to change the environment.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reticulate)\n#fill in \"name\"\nuse_python(“C:/Users/name/anaconda3/envs/r-tensorflow/python.exe”, required=TRUE)\npy_config()\n#test\nlibrary(tensorflow)\ntf$constant(“TensorfFlow is Working!”)\n```\n:::\n\n\n\n\n**Option 2.** Install `tensorflow` from within RStudio. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"tensorflow\")\ntensorflow::install_tensorflow()\n\nlibrary(tensorflow)\ntf$constant(\"TensorFlow is working!\")\n\nlibrary(reticulate)\npy_config()\n```\n:::\n\n\n\n\nI had trouble installing and running `tensorflow`. Due to some decisions I made along the way, I now require the additional code:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reticulate)\nreticulate::use_python(\"C:/Users/neil_/anaconda3/python.exe\")\n```\n:::\n\n\n\n\n**I hope you don't have the same trouble!**\n\nAfter successfully installing `tensorflow` load the libraries. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nlibrary(ggplot2)\n```\n:::\n\n\n\n\nWe will use a data that comes with R: \"Boston\". It 506 observations and 14 variables. The outcome we aim to predict is \"medv\": the median value of owner occupied homes (in '000s dollars). The number of predictors is $p=13$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the Boston dataset\nlibrary(MASS)\ndata <- Boston \n```\n:::\n\n\n\n\n\n\n## Create training and testing database\n\nAs in Seminar 2, we need to split the data into a training and testing sample. Take note of the normalization step. We missed this step in Seminar 2. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(6)\nx <- as.matrix(data[, -ncol(data)])  # All columns except the last (predictors)\ny <- as.numeric(data[, ncol(data)])  # The last column\nx <- scale(x) # Normalize the predictors\n\n# Split the data\ntest_proportion <- 0.2 # Define the proportion of the test set\nn <- nrow(data)\ntest_indices <- sample(1:n, size = floor(test_proportion * n)) \ntrain_indices <- setdiff(1:n, test_indices) # find all indices that are not in test_indices\nx_train <- as.matrix(x[train_indices, ])\ny_train <- as.numeric(y[train_indices])\nx_test <- as.matrix(x[test_indices, ])\ny_test <- as.numeric(y[test_indices])\n```\n:::\n\n\n\n\n## Execute Feedforward Neural Network\n\nTo begin, we will execute a model with a 128-128-128 architecture: \n\n- width $q=128$;\n- depth $r=3$;\n- activation function $g = ReLU$\n\nThis means that the number of parameters (weights) will be,\n\n$$\n  \\underbrace{(13+1)\\cdot128}_\\text{Layer 1} + \\underbrace{(128+1)\\cdot128}_\\text{Layer 2} + \\underbrace{(128+1)\\cdot128}_\\text{Layer 3}+\\underbrace{128+1}_\\text{Output Layer} = 34,945\n$$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Input layer\ninput <- layer_input(shape = c(ncol(x_train)))  \n\noutput <- input %>%\n  layer_dense(units = 128, activation = \"relu\") %>%\n  layer_dropout(rate = 0.5) %>%\n  layer_dense(units = 128, activation = \"relu\") %>%\n  layer_dropout(rate = 0.5) %>%\n  layer_dense(units = 128, activation = \"relu\") %>%\n  layer_dropout(rate = 0.5) %>%\n  layer_dense(units = 1)  # Single output layer\n\nmodel <- keras_model(inputs = input, outputs = output)\n\n# Configure the model\ntensorflow::tf$keras$Model$compile(\n  model,\n  loss = \"mse\",  # Mean Squared Error\n  optimizer = tensorflow::tf$keras$optimizers$Adam(),\n  metrics = list(\"mae\")  # Mean Absolute Error\n) \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Train the model\nhistory <- tensorflow::tf$keras$Model$fit(\n  model,\n  x = tensorflow::tf$convert_to_tensor(x_train),  # Convert x to TensorFlow Tensor\n  y = tensorflow::tf$convert_to_tensor(y_train),  # Convert y to TensorFlow Tensor\n  epochs = 50L,  # Number of epochs, L indicates integer; \n  batch_size = 32L,  # Batch size\n  validation_split = 0.2  # Use 20% of the data for validation\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEpoch 1/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 2s/step - loss: 689.5551 - mae: 24.4692\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 628.0851 - mae: 23.2673 - val_loss: 231.1659 - val_mae: 14.2423\nEpoch 2/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 490.0184 - mae: 20.1199\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 495.0543 - mae: 20.3299 - val_loss: 141.2276 - val_mae: 10.3903\nEpoch 3/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 331.0157 - mae: 16.0697\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 264.7611 - mae: 13.9956 - val_loss: 60.5894 - val_mae: 6.4758\nEpoch 4/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 101.1181 - mae: 7.9549\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 99.1883 - mae: 7.8011 - val_loss: 47.5501 - val_mae: 5.7522\nEpoch 5/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 71.2275 - mae: 5.9820\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 96.5553 - mae: 7.0774 - val_loss: 45.2462 - val_mae: 5.4967\nEpoch 6/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 39.8872 - mae: 5.3088\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 66.2081 - mae: 6.3266 - val_loss: 41.6517 - val_mae: 5.2288\nEpoch 7/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 50.0161 - mae: 5.0964\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 62.2262 - mae: 5.8205 - val_loss: 38.1698 - val_mae: 4.9880\nEpoch 8/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 44.1734 - mae: 5.2707\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 58.2122 - mae: 5.7769 - val_loss: 37.1171 - val_mae: 4.9102\nEpoch 9/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 56.5878 - mae: 5.4311\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 56.0823 - mae: 5.7323 - val_loss: 34.1220 - val_mae: 4.7215\nEpoch 10/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 64.6227 - mae: 6.7946\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 55.5275 - mae: 5.8702 - val_loss: 33.4966 - val_mae: 4.6757\nEpoch 11/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 32.8004 - mae: 4.5381\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 58.2897 - mae: 5.6734 - val_loss: 33.2381 - val_mae: 4.6394\nEpoch 12/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 35.0458 - mae: 4.6946\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 53.3897 - mae: 5.4994 - val_loss: 30.9257 - val_mae: 4.4722\nEpoch 13/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 82.0047 - mae: 6.7336\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 60.4770 - mae: 5.9668 - val_loss: 27.5460 - val_mae: 4.1768\nEpoch 14/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 48.4893 - mae: 5.7550\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 48.9833 - mae: 5.4841 - val_loss: 29.2070 - val_mae: 4.3032\nEpoch 15/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 34.4505 - mae: 4.9218\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 51.1002 - mae: 5.5813 - val_loss: 25.9550 - val_mae: 4.0207\nEpoch 16/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 38.9243 - mae: 5.0070\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 47.4817 - mae: 5.2076 - val_loss: 25.9668 - val_mae: 3.9739\nEpoch 17/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 86.5412 - mae: 6.5236\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 54.2851 - mae: 5.4987 - val_loss: 22.9260 - val_mae: 3.6931\nEpoch 18/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 41.1477 - mae: 5.2850\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 51.9624 - mae: 5.5908 - val_loss: 23.0947 - val_mae: 3.6264\nEpoch 19/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 45.1620 - mae: 5.2506\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 43.3628 - mae: 4.9506 - val_loss: 23.7447 - val_mae: 3.6919\nEpoch 20/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 21.4861 - mae: 3.9764\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 41.6584 - mae: 4.8637 - val_loss: 21.1909 - val_mae: 3.4261\nEpoch 21/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 29.3402 - mae: 4.2791\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 42.7252 - mae: 4.8300 - val_loss: 21.8536 - val_mae: 3.4931\nEpoch 22/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 49.0534 - mae: 5.3895\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 44.9081 - mae: 5.1512 - val_loss: 20.2437 - val_mae: 3.3453\nEpoch 23/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 29.3735 - mae: 4.3542\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 35.9293 - mae: 4.6903 - val_loss: 19.1758 - val_mae: 3.3033\nEpoch 24/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 42.6983 - mae: 4.8196\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 54.8558 - mae: 5.5861 - val_loss: 21.2709 - val_mae: 3.4362\nEpoch 25/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 30.1738 - mae: 4.5161\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 42.3444 - mae: 4.9715 - val_loss: 21.4934 - val_mae: 3.4464\nEpoch 26/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 35.3122 - mae: 4.7983\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 39.7620 - mae: 4.9163 - val_loss: 21.0186 - val_mae: 3.3847\nEpoch 27/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 26.5361 - mae: 4.1525\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 47.9204 - mae: 5.0488 - val_loss: 20.2418 - val_mae: 3.3162\nEpoch 28/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 99.7565 - mae: 8.1418\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 57.6950 - mae: 5.9128 - val_loss: 22.8760 - val_mae: 3.6585\nEpoch 29/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 39.5229 - mae: 4.9653\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 42.3745 - mae: 4.9169 - val_loss: 23.6032 - val_mae: 3.7503\nEpoch 30/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 49.5030 - mae: 5.3322\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 45.3675 - mae: 5.1060 - val_loss: 23.3391 - val_mae: 3.7161\nEpoch 31/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 34.5553 - mae: 4.4793\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 43.1995 - mae: 4.9744 - val_loss: 18.4998 - val_mae: 3.1744\nEpoch 32/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 28.7278 - mae: 4.5667\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 36.7190 - mae: 4.7354 - val_loss: 18.2170 - val_mae: 3.1817\nEpoch 33/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 28.6269 - mae: 4.2149\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 44.9428 - mae: 5.0375 - val_loss: 20.0228 - val_mae: 3.3484\nEpoch 34/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 57.2449 - mae: 6.0325\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 44.3474 - mae: 5.2479 - val_loss: 18.8312 - val_mae: 3.2175\nEpoch 35/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 44.3587 - mae: 5.3770\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 42.7099 - mae: 4.9043 - val_loss: 18.9102 - val_mae: 3.2459\nEpoch 36/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 45.6255 - mae: 5.3796\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 39.0158 - mae: 4.7060 - val_loss: 18.6573 - val_mae: 3.2269\nEpoch 37/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 38.3910 - mae: 4.7404\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 42.0344 - mae: 4.8988 - val_loss: 17.5208 - val_mae: 3.0985\nEpoch 38/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 25.7330 - mae: 4.1183\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 41.3704 - mae: 4.9453 - val_loss: 16.7105 - val_mae: 3.0033\nEpoch 39/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 36.5605 - mae: 4.4369\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 40.0485 - mae: 4.7192 - val_loss: 17.0620 - val_mae: 3.0374\nEpoch 40/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 50.0592 - mae: 5.2442\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 43.4579 - mae: 5.0555 - val_loss: 18.0735 - val_mae: 3.1809\nEpoch 41/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 35.1892 - mae: 4.6067\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 33.4224 - mae: 4.4041 - val_loss: 17.8846 - val_mae: 3.1640\nEpoch 42/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 50.6706 - mae: 5.3024\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 41.5095 - mae: 4.7731 - val_loss: 16.2472 - val_mae: 2.9724\nEpoch 43/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 53.5104 - mae: 5.5309\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 42.0559 - mae: 4.8937 - val_loss: 16.7101 - val_mae: 3.0012\nEpoch 44/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 36.4219 - mae: 4.6888\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 42.1914 - mae: 4.8741 - val_loss: 16.8998 - val_mae: 3.0745\nEpoch 45/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 33.3051 - mae: 4.8560\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 42.2671 - mae: 4.9946 - val_loss: 16.3686 - val_mae: 2.9832\nEpoch 46/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 54.5692 - mae: 5.0043\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 38.9898 - mae: 4.5786 - val_loss: 15.9083 - val_mae: 2.9621\nEpoch 47/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 32.0412 - mae: 4.4798\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 37.9167 - mae: 4.6615 - val_loss: 17.2492 - val_mae: 3.1539\nEpoch 48/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 46.9966 - mae: 5.2159\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 48.7749 - mae: 4.9650 - val_loss: 15.7270 - val_mae: 2.9691\nEpoch 49/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 48.7861 - mae: 4.3676\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 36.9421 - mae: 4.3206 - val_loss: 15.6015 - val_mae: 2.9523\nEpoch 50/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 49.5210 - mae: 4.9202\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 39.6472 - mae: 4.7467 - val_loss: 16.3102 - val_mae: 3.0015\n```\n\n\n:::\n\n```{.r .cell-code}\n# Predictions\npredictions <- tensorflow::tf$keras$Model$predict(model, tensorflow::tf$convert_to_tensor(x_test))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n```\n\n\n:::\n\n```{.r .cell-code}\n# Print the result\ntensorflow::tf$keras$Model$summary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel: \"functional\"\n┌─────────────────────────────────┬────────────────────────┬───────────────┐\n│ Layer (type)                    │ Output Shape           │       Param # │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ input_layer (InputLayer)        │ (None, 13)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (Dense)                 │ (None, 128)            │         1,792 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (Dropout)             │ (None, 128)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (Dense)                 │ (None, 128)            │        16,512 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (Dropout)             │ (None, 128)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 128)            │        16,512 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (Dropout)               │ (None, 128)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (Dense)                   │ (None, 1)              │           129 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 104,837 (409.52 KB)\n Trainable params: 34,945 (136.50 KB)\n Non-trainable params: 0 (0.00 B)\n Optimizer params: 69,892 (273.02 KB)\n```\n\n\n:::\n:::\n\n\n\n\nExtract the final training and validation loss\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_training_loss <- history$history$loss[length(history$history$loss)]\nfinal_validation_loss <- history$history$val_loss[length(history$history$val_loss)]\n\ncat(\"Final Training Loss:\", final_training_loss, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFinal Training Loss: 38.97892 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Final Validation Loss:\", final_validation_loss, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFinal Validation Loss: 16.31018 \n```\n\n\n:::\n:::\n\n\n\n\n## Visualize the results\nConvert history to a data frame with epoch numbers\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhistory_df <- as.data.frame(history$history)\nhistory_df$epoch <- seq_len(nrow(history_df))\n```\n:::\n\n\n\n\nPlot training and validation loss\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(history_df, aes(x = epoch)) +\n  geom_line(aes(y = loss, color = \"Training Loss\"), linewidth = 1) +\n  geom_line(aes(y = val_loss, color = \"Validation Loss\"), linewidth = 1) +\n  labs(\n    title = \"Training and Validation Loss\",\n    x = \"Epoch\",\n    y = \"Loss\"\n  ) +\n  scale_color_manual(values = c(\"blue\", \"red\")) +\n  theme_minimal() +\n  theme(\n    legend.title = element_blank(),\n    legend.position = \"bottom\"  # Options: \"top\", \"bottom\", \"left\", \"right\", or c(x, y) for custom\n  )\n```\n\n::: {.cell-output-display}\n![](seminar-3_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n\nPlot training and validation mean absolute error (MAE)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(history_df, aes(x = epoch)) +\n  geom_line(aes(y = mae, color = \"Training MAE\"), linewidth = 1) +\n  geom_line(aes(y = val_mae, color = \"Validation MAE\"), linewidth = 1) +\n  labs(\n    title = \"Training and Validation Mean Absolute Error\",\n    x = \"Epoch\",\n    y = \"Mean Absolute Error\"\n  ) +\n  scale_color_manual(values = c(\"blue\", \"red\")) +\n  theme_minimal() +\n  theme(\n    legend.title = element_blank(),\n    legend.position = \"bottom\"  # Options: \"top\", \"bottom\", \"left\", \"right\", or c(x, y) for custom\n  )\n```\n\n::: {.cell-output-display}\n![](seminar-3_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "seminar-3_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}