{
  "hash": "434b77eaee745615066397fdb563d5ec",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Seminar 3\"\nformat: html\n---\n\n\n\n\n## Overview\n\nThe goal of seminar 3 is to review the questions in Problem Set 2. Many of these questions do no require R and a selection will be discussed in person during class. Here, you will find an initial attempt at Q4.\n\n**Question 4:** Please try working through the coding example in the week 6's lecture note on the feedforward neural network.\n\n::: {#exr-q4}\nTry to run the following code on your own computer. You should be able to replicate the results in the slides for a 128-128-128 architecture. Next, modify the code to replicate other architectures.  \n:::\n\n## Load packages and data\n\n::: {.callout-warning title=\"Updating R and installing `tensorflow`\"}\nI had issues with some of the packages and needed to update to the latest version of R (4.2.2.). In addition, you will need to have `tensorflow` installed with Python on your computer. To do so, open the Anaconda Prompt and type `pip install tensorflow`.   \n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nlibrary(ggplot2)\n```\n:::\n\n\n\n\nWe will use a data that comes with R: \"Boston\". It 506 observations and 14 variables. The outcome we aim to predict is \"medv\": the median value of owner occupied homes (in '000s dollars). The number of predictors is $p=13$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the Boston dataset\nlibrary(MASS)\ndata <- Boston \n```\n:::\n\n\n\n\nI had trouble installing and running `tensorflow`. I've added a discussion of my troubleshooting journey to the end. Due to some decisions I made along the way (suggested by ChatGPT), I now require the additional code:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reticulate)\nreticulate::use_python(\"C:/Users/neil_/anaconda3/python.exe\")\n```\n:::\n\n\n\n\n**I hope you don't have the same trouble!**\n\n## Create training and testing database\n\nAs in Seminar 2, we need to split the data into a training and testing sample. Take note of the normalization step. We missed this step in Seminar 2. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(6)\nx <- as.matrix(data[, -ncol(data)])  # All columns except the last (predictors)\ny <- as.numeric(data[, ncol(data)])  # The last column\nx <- scale(x) # Normalize the predictors\n\n# Split the data\ntest_proportion <- 0.2 # Define the proportion of the test set\nn <- nrow(data)\ntest_indices <- sample(1:n, size = floor(test_proportion * n)) \ntrain_indices <- setdiff(1:n, test_indices) # find all indices that are not in test_indices\nx_train <- as.matrix(x[train_indices, ])\ny_train <- as.numeric(y[train_indices])\nx_test <- as.matrix(x[test_indices, ])\ny_test <- as.numeric(y[test_indices])\n```\n:::\n\n\n\n\n## Execute Feedforward Neural Network\n\nTo begin, we will execute a model with a 128-128-128 architecture: \n\n- width $q=128$;\n- depth $r=3$;\n- activation function $g = ReLU$\n\nThis means that the number of parameters (weights) will be,\n\n$$\n  \\underbrace{(13+1)\\cdot128}_\\text{Layer 1} + \\underbrace{(128+1)\\cdot128}_\\text{Layer 2} + \\underbrace{(128+1)\\cdot128}_\\text{Layer 3}+\\underbrace{128+1}_\\text{Output Layer} = 34,945\n$$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Input layer\ninput <- layer_input(shape = c(ncol(x_train)))  \n\noutput <- input %>%\n  layer_dense(units = 128, activation = \"relu\") %>%\n  layer_dropout(rate = 0.5) %>%\n  layer_dense(units = 128, activation = \"relu\") %>%\n  layer_dropout(rate = 0.5) %>%\n  layer_dense(units = 128, activation = \"relu\") %>%\n  layer_dropout(rate = 0.5) %>%\n  layer_dense(units = 1)  # Single output layer\n\nmodel <- keras_model(inputs = input, outputs = output)\n\n# Configure the model\ntensorflow::tf$keras$Model$compile(\n  model,\n  loss = \"mse\",  # Mean Squared Error\n  optimizer = tensorflow::tf$keras$optimizers$Adam(),\n  metrics = list(\"mae\")  # Mean Absolute Error\n) \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Train the model\nhistory <- tensorflow::tf$keras$Model$fit(\n  model,\n  x = tensorflow::tf$convert_to_tensor(x_train),  # Convert x to TensorFlow Tensor\n  y = tensorflow::tf$convert_to_tensor(y_train),  # Convert y to TensorFlow Tensor\n  epochs = 50L,  # Number of epochs, L indicates integer; \n  batch_size = 32L,  # Batch size\n  validation_split = 0.2  # Use 20% of the data for validation\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEpoch 1/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 3s/step - loss: 663.9166 - mae: 24.0856\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 632.0574 - mae: 23.4652 - val_loss: 245.5663 - val_mae: 14.7091\nEpoch 2/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 507.9779 - mae: 20.5717\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 528.3538 - mae: 20.9877 - val_loss: 173.0639 - val_mae: 11.7214\nEpoch 3/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 396.1314 - mae: 17.5856\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 339.7480 - mae: 16.2843 - val_loss: 88.6512 - val_mae: 7.8858\nEpoch 4/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 142.9713 - mae: 10.2946\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 141.5338 - mae: 9.5501 - val_loss: 55.9873 - val_mae: 6.3774\nEpoch 5/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 105.8257 - mae: 7.5125\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 131.5654 - mae: 8.4772 - val_loss: 47.3971 - val_mae: 5.7636\nEpoch 6/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 69.9026 - mae: 6.7872\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 83.1421 - mae: 7.0959 - val_loss: 49.9955 - val_mae: 5.7943\nEpoch 7/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 85.2274 - mae: 7.1684\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 74.1852 - mae: 6.6323 - val_loss: 46.2560 - val_mae: 5.4925\nEpoch 8/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 110.4753 - mae: 8.2282\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 76.9050 - mae: 6.5317 - val_loss: 33.8680 - val_mae: 4.7051\nEpoch 9/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 58.7008 - mae: 5.6691\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 64.0665 - mae: 5.8647 - val_loss: 30.4828 - val_mae: 4.4013\nEpoch 10/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 44.0340 - mae: 5.3486\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 58.8389 - mae: 5.7397 - val_loss: 30.1241 - val_mae: 4.3410\nEpoch 11/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 35.7562 - mae: 4.7363\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 52.5509 - mae: 5.4408 - val_loss: 30.6483 - val_mae: 4.3603\nEpoch 12/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 76.4808 - mae: 6.2749\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 56.8772 - mae: 5.6181 - val_loss: 28.6776 - val_mae: 4.2353\nEpoch 13/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 36.8314 - mae: 4.8038\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 46.0775 - mae: 5.0009 - val_loss: 28.4119 - val_mae: 4.2491\nEpoch 14/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 82.2864 - mae: 6.8742\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 56.8664 - mae: 5.6361 - val_loss: 29.8954 - val_mae: 4.3593\nEpoch 15/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 34.5181 - mae: 4.6077\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 60.0347 - mae: 5.6449 - val_loss: 26.2478 - val_mae: 4.1056\nEpoch 16/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 28.3506 - mae: 4.2934\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 44.9850 - mae: 5.0919 - val_loss: 25.0533 - val_mae: 3.9908\nEpoch 17/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 38.3009 - mae: 5.1667\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 48.8275 - mae: 5.2247 - val_loss: 23.8041 - val_mae: 3.8546\nEpoch 18/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 46.9098 - mae: 4.7829\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 53.1007 - mae: 5.4452 - val_loss: 22.7611 - val_mae: 3.8064\nEpoch 19/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 37.0184 - mae: 4.9325\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 41.8245 - mae: 5.0733 - val_loss: 22.6200 - val_mae: 3.8002\nEpoch 20/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 32.9736 - mae: 4.7559\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 45.2044 - mae: 5.1534 - val_loss: 22.2065 - val_mae: 3.7493\nEpoch 21/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 40.8096 - mae: 4.8859\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 44.2308 - mae: 5.0525 - val_loss: 21.2324 - val_mae: 3.6642\nEpoch 22/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 37.8849 - mae: 4.2407\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 48.8046 - mae: 4.9385 - val_loss: 20.1995 - val_mae: 3.6009\nEpoch 23/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 87.2823 - mae: 6.8691\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 61.1699 - mae: 5.7499 - val_loss: 19.2973 - val_mae: 3.4519\nEpoch 24/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 45.5278 - mae: 5.4121\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 41.1869 - mae: 5.0334 - val_loss: 19.3923 - val_mae: 3.4728\nEpoch 25/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 35.5219 - mae: 3.8203\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 46.0599 - mae: 4.9638 - val_loss: 19.1405 - val_mae: 3.4555\nEpoch 26/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 31.1827 - mae: 4.8080\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 45.7156 - mae: 5.2385 - val_loss: 19.9198 - val_mae: 3.5578\nEpoch 27/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 31.1750 - mae: 4.5761\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 41.2889 - mae: 4.9923 - val_loss: 21.1100 - val_mae: 3.6708\nEpoch 28/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 87.4780 - mae: 5.6919\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 62.9454 - mae: 5.2592 - val_loss: 18.4326 - val_mae: 3.3825\nEpoch 29/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 37.3212 - mae: 4.8266\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 40.8122 - mae: 4.8955 - val_loss: 18.5076 - val_mae: 3.4074\nEpoch 30/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 30.6347 - mae: 4.1556\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 38.9192 - mae: 4.5772 - val_loss: 19.8438 - val_mae: 3.5056\nEpoch 31/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 43.7283 - mae: 4.8657\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 38.9090 - mae: 4.5786 - val_loss: 19.6350 - val_mae: 3.4969\nEpoch 32/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 61.5954 - mae: 6.3124\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 52.5210 - mae: 5.7408 - val_loss: 18.8119 - val_mae: 3.3884\nEpoch 33/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 27.0851 - mae: 4.0869\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 46.4488 - mae: 5.1246 - val_loss: 18.3383 - val_mae: 3.3172\nEpoch 34/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 134.5245 - mae: 7.2331\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 67.2342 - mae: 5.4009 - val_loss: 17.5370 - val_mae: 3.2379\nEpoch 35/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 39.8166 - mae: 5.1745\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 41.6042 - mae: 4.9993 - val_loss: 18.9474 - val_mae: 3.3788\nEpoch 36/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 31.2738 - mae: 4.2572\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 40.7058 - mae: 4.9300 - val_loss: 18.1934 - val_mae: 3.3237\nEpoch 37/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 22.5861 - mae: 3.6045\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 38.8193 - mae: 4.6183 - val_loss: 16.4218 - val_mae: 3.1274\nEpoch 38/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 21.3022 - mae: 3.9133\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 39.4454 - mae: 4.9131 - val_loss: 17.0294 - val_mae: 3.2152\nEpoch 39/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 68.0831 - mae: 5.4992\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 44.3206 - mae: 4.7629 - val_loss: 16.6888 - val_mae: 3.1837\nEpoch 40/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 74.1833 - mae: 5.6997\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 52.8883 - mae: 5.2475 - val_loss: 16.3902 - val_mae: 3.1227\nEpoch 41/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 46.1044 - mae: 5.2085\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 40.7309 - mae: 4.8950 - val_loss: 16.5630 - val_mae: 3.1448\nEpoch 42/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 26.9159 - mae: 4.3570\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 33.4741 - mae: 4.5750 - val_loss: 16.9832 - val_mae: 3.1910\nEpoch 43/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 46.8199 - mae: 5.5385\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 46.9350 - mae: 5.1078 - val_loss: 17.0056 - val_mae: 3.2278\nEpoch 44/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 24.3247 - mae: 3.6715\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 35.3725 - mae: 4.5227 - val_loss: 17.7155 - val_mae: 3.3111\nEpoch 45/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 23.4719 - mae: 3.8599\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 32.9006 - mae: 4.3945 - val_loss: 18.4533 - val_mae: 3.3899\nEpoch 46/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 33.6802 - mae: 4.7497\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 35.2375 - mae: 4.4376 - val_loss: 16.6232 - val_mae: 3.1828\nEpoch 47/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 50.0619 - mae: 5.4806\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 46.1781 - mae: 5.1967 - val_loss: 16.9815 - val_mae: 3.2123\nEpoch 48/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 45.3129 - mae: 4.8168\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 38.7393 - mae: 4.7289 - val_loss: 17.6066 - val_mae: 3.2604\nEpoch 49/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 37.5335 - mae: 4.5116\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 43.7944 - mae: 4.7757 - val_loss: 15.9966 - val_mae: 3.1041\nEpoch 50/50\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 39.1864 - mae: 4.1038\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 38.2058 - mae: 4.5389 - val_loss: 17.1638 - val_mae: 3.2152\n```\n\n\n:::\n\n```{.r .cell-code}\n# Predictions\npredictions <- tensorflow::tf$keras$Model$predict(model, tensorflow::tf$convert_to_tensor(x_test))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n```\n\n\n:::\n\n```{.r .cell-code}\n# Print the result\ntensorflow::tf$keras$Model$summary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel: \"functional\"\n┌─────────────────────────────────┬────────────────────────┬───────────────┐\n│ Layer (type)                    │ Output Shape           │       Param # │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ input_layer (InputLayer)        │ (None, 13)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (Dense)                 │ (None, 128)            │         1,792 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (Dropout)             │ (None, 128)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (Dense)                 │ (None, 128)            │        16,512 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (Dropout)             │ (None, 128)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 128)            │        16,512 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (Dropout)               │ (None, 128)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (Dense)                   │ (None, 1)              │           129 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 104,837 (409.52 KB)\n Trainable params: 34,945 (136.50 KB)\n Non-trainable params: 0 (0.00 B)\n Optimizer params: 69,892 (273.02 KB)\n```\n\n\n:::\n:::\n\n\n\n\nExtract the final training and validation loss\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_training_loss <- history$history$loss[length(history$history$loss)]\nfinal_validation_loss <- history$history$val_loss[length(history$history$val_loss)]\n\ncat(\"Final Training Loss:\", final_training_loss, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFinal Training Loss: 33.58537 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Final Validation Loss:\", final_validation_loss, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFinal Validation Loss: 17.16378 \n```\n\n\n:::\n:::\n\n\n\n\n## Visualize the results\nConvert history to a data frame with epoch numbers\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhistory_df <- as.data.frame(history$history)\nhistory_df$epoch <- seq_len(nrow(history_df))\n```\n:::\n\n\n\n\nPlot training and validation loss\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(history_df, aes(x = epoch)) +\n  geom_line(aes(y = loss, color = \"Training Loss\"), linewidth = 1) +\n  geom_line(aes(y = val_loss, color = \"Validation Loss\"), linewidth = 1) +\n  labs(\n    title = \"Training and Validation Loss\",\n    x = \"Epoch\",\n    y = \"Loss\"\n  ) +\n  scale_color_manual(values = c(\"blue\", \"red\")) +\n  theme_minimal() +\n  theme(\n    legend.title = element_blank(),\n    legend.position = \"bottom\"  # Options: \"top\", \"bottom\", \"left\", \"right\", or c(x, y) for custom\n  )\n```\n\n::: {.cell-output-display}\n![](seminar-3_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n\nPlot training and validation mean absolute error (MAE)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(history_df, aes(x = epoch)) +\n  geom_line(aes(y = mae, color = \"Training MAE\"), linewidth = 1) +\n  geom_line(aes(y = val_mae, color = \"Validation MAE\"), linewidth = 1) +\n  labs(\n    title = \"Training and Validation Mean Absolute Error\",\n    x = \"Epoch\",\n    y = \"Mean Absolute Error\"\n  ) +\n  scale_color_manual(values = c(\"blue\", \"red\")) +\n  theme_minimal() +\n  theme(\n    legend.title = element_blank(),\n    legend.position = \"bottom\"  # Options: \"top\", \"bottom\", \"left\", \"right\", or c(x, y) for custom\n  )\n```\n\n::: {.cell-output-display}\n![](seminar-3_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n\n## Troubleshooting\n\nI struggled initially to run the following code. In the end, I used ChatGPT to help me work through a series errors, but found that ChatGPTs solutions created new problems that I struggled to undo. \n\n::: {.callout-warning title=\"PC vs Mac\"}\nI am PC user, so the following advice may not relate to the experience of a Mac user.  \n:::\n\nHere's what I did:`\n\n1. Initially, I installed `tensorflow` and `keras` in R (via RStudio).\n2. I then noted that the packages required R4.2.2 and I was on R4.2.0. So, I updated R; which essentially requires installing R again. RStudio refused to recognize the new version, so I had to manually do so using the following advice from ChatGPT: \n\n- Close RStudio.\n- Open RStudio, but hold Ctrl (Windows) or Cmd (Mac) while clicking the icon.\n- A menu should appear allowing you to select the new R installation.\n\nI then used the following code to shift my packages:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run in NEW version of R (not RStudio)\nold_lib <- \"C:/Program Files/R/R-4.4.0/library\"\nnew_lib <- \"C:/Program Files/R/R-4.4.2/library\"\ndir.create(new_lib, showWarnings = FALSE)\nfile.copy(list.files(old_lib, full.names = TRUE), new_lib, recursive = TRUE)\n```\n:::\n\n\n\n\nType `version` into the RStudio console to check that the up-to-date version is being used. \n\n3. As soon as I tried to run the code I got an error, the beginning of which read:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nError: Valid installation of TensorFlow not found.\n\nPython environments searched for 'tensorflow' package:\n C:\\Users\\neil_\\anaconda3\\python.exe\n```\n:::\n\n\n\n\nThis is where things started to go wrong. \n\n4. You can use the following code in RStudio to check your python environment:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reticulate)\npy_config()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\npython:         C:/Users/neil_/anaconda3/python.exe\nlibpython:      C:/Users/neil_/anaconda3/python312.dll\npythonhome:     C:/Users/neil_/anaconda3\nversion:        3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 15:03:56) [MSC v.1929 64 bit (AMD64)]\nArchitecture:   64bit\nnumpy:          C:/Users/neil_/anaconda3/Lib/site-packages/numpy\nnumpy_version:  1.26.4\ntensorflow:     C:\\Users\\neil_\\ANACON~1\\Lib\\site-packages\\tensorflow\\__init__.p\n\nNOTE: Python version was forced by use_python() function\n```\n\n\n:::\n:::\n\n\n\n\nI followed ChatGPTs suggestion to \"Open Anaconda Prompt (or your command prompt) and run:\"\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconda activate base\nconda install tensorflow\n```\n:::\n\n\n\n\nThis gave me a long error. Something like, \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nChannels:\n - defaults\nPlatform: win-64\nCollecting package metadata (repodata.json): done\nSolving environment: - warning  libmamba Added empty dependency for problem type SOLVER_RULE_UPDATE\nfailed\n\nLibMambaUnsatisfiableError: Encountered problems while solving:\n  - nothing provides bleach 1.5.0 needed by tensorboard-1.7.0-py35he025d50_1\n```\n:::\n\n\n\n\n\n5. According to ChatGPT, \"The error suggests that there’s a Python version mismatch when trying to install TensorFlow in your Anaconda environment. It looks like your Anaconda environment has Python 3.12, but TensorFlow requires Python 3.8–3.10 (depending on the version).\" So, it suggested creating a virtual environment with Python 3.10. Using the code:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconda create --name r-tensorflow python=3.10\nconda activate r-tensorflow\npip install tensorflow\n```\n:::\n\n\n\n\nYou then need to configure R to use this new environment. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reticulate)\nuse_condaenv(\"r-tensorflow\", required = TRUE)\npy_config()\ninstall.packages(\"tensorflow\")\ntensorflow::install_tensorflow()\nlibrary(tensorflow)\ntf$constant(\"TensorFlow is working!\")\n```\n:::\n\n\n\n\n6. I eventually had to undo all of this. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconda info --envs\nconda deactivate\nconda env remove --name r-tensorflow\nconda info --envs\nconda clean --all\n```\n:::\n\n\n\n\nAND instead, managed to install `tensorflow` in the Base environment using\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npip install tensorflow\n```\n:::\n\n\n\n\n7. For some reason, I could not get Quarto to stop looking in the `r-tensorflow` environment. So, now the code runs only if I add:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reticulate)\nreticulate::use_python(\"C:/Users/neil_/anaconda3/python.exe\")\n```\n:::",
    "supporting": [
      "seminar-3_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}